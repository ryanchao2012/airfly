# Auto generated by 'inv collect-airflow'
from airfly._vendor.abc import ABC
from airfly._vendor.airflow.models.baseoperator import BaseOperator


class DatabricksCreateJobsOperator(BaseOperator):
    json: "Any | None"
    name: "str | None"
    description: "str | None"
    tags: "dict[str, str] | None"
    tasks: "list[dict] | None"
    job_clusters: "list[dict] | None"
    email_notifications: "dict | None"
    webhook_notifications: "dict | None"
    notification_settings: "dict | None"
    timeout_seconds: "int | None"
    schedule: "dict | None"
    max_concurrent_runs: "int | None"
    git_source: "dict | None"
    access_control_list: "list[dict] | None"
    databricks_conn_id: "str"
    polling_period_seconds: "int"
    databricks_retry_limit: "int"
    databricks_retry_delay: "int"
    databricks_retry_args: "dict[Any, Any] | None"


class DatabricksSubmitRunOperator(BaseOperator):
    json: "Any | None"
    tasks: "list[object] | None"
    spark_jar_task: "dict[str, str] | None"
    notebook_task: "dict[str, str] | None"
    spark_python_task: "dict[str, str | list[str]] | None"
    spark_submit_task: "dict[str, list[str]] | None"
    pipeline_task: "dict[str, str] | None"
    dbt_task: "dict[str, str | list[str]] | None"
    new_cluster: "dict[str, object] | None"
    existing_cluster_id: "str | None"
    libraries: "list[dict[str, Any]] | None"
    run_name: "str | None"
    timeout_seconds: "int | None"
    databricks_conn_id: "str"
    polling_period_seconds: "int"
    databricks_retry_limit: "int"
    databricks_retry_delay: "int"
    databricks_retry_args: "dict[Any, Any] | None"
    do_xcom_push: "bool"
    idempotency_token: "str | None"
    access_control_list: "list[dict[str, str]] | None"
    wait_for_termination: "bool"
    git_source: "dict[str, str] | None"
    deferrable: "bool"


class DatabricksSubmitRunDeferrableOperator(DatabricksSubmitRunOperator):
    pass


class DatabricksRunNowOperator(BaseOperator):
    job_id: "str | None"
    job_name: "str | None"
    json: "Any | None"
    notebook_params: "dict[str, str] | None"
    python_params: "list[str] | None"
    jar_params: "list[str] | None"
    spark_submit_params: "list[str] | None"
    python_named_params: "dict[str, str] | None"
    idempotency_token: "str | None"
    databricks_conn_id: "str"
    polling_period_seconds: "int"
    databricks_retry_limit: "int"
    databricks_retry_delay: "int"
    databricks_retry_args: "dict[Any, Any] | None"
    do_xcom_push: "bool"
    wait_for_termination: "bool"
    deferrable: "bool"
    repair_run: "bool"
    cancel_previous_runs: "bool"


class DatabricksRunNowDeferrableOperator(DatabricksRunNowOperator):
    pass


class DatabricksTaskBaseOperator(BaseOperator, ABC):
    caller: "str"
    databricks_conn_id: "str"
    databricks_retry_args: "dict[Any, Any] | None"
    databricks_retry_delay: "int"
    databricks_retry_limit: "int"
    deferrable: "bool"
    existing_cluster_id: "str"
    job_cluster_key: "str"
    new_cluster: "dict[str, Any] | None"
    polling_period_seconds: "int"
    wait_for_termination: "bool"
    workflow_run_metadata: "dict[str, Any] | None"


class DatabricksNotebookOperator(DatabricksTaskBaseOperator):
    notebook_path: "str"
    source: "str"
    databricks_conn_id: "str"
    databricks_retry_args: "dict[Any, Any] | None"
    databricks_retry_delay: "int"
    databricks_retry_limit: "int"
    deferrable: "bool"
    existing_cluster_id: "str"
    job_cluster_key: "str"
    new_cluster: "dict[str, Any] | None"
    notebook_packages: "list[dict[str, Any]] | None"
    notebook_params: "dict | None"
    polling_period_seconds: "int"
    wait_for_termination: "bool"
    workflow_run_metadata: "dict | None"


class DatabricksTaskOperator(DatabricksTaskBaseOperator):
    task_config: "dict"
    databricks_conn_id: "str"
    databricks_retry_args: "dict[Any, Any] | None"
    databricks_retry_delay: "int"
    databricks_retry_limit: "int"
    deferrable: "bool"
    existing_cluster_id: "str"
    job_cluster_key: "str"
    new_cluster: "dict[str, Any] | None"
    polling_period_seconds: "int"
    wait_for_termination: "bool"
    workflow_run_metadata: "dict | None"
